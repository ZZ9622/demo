import os
import torch
import numpy as np
from PIL import Image
from transformers import (
    AutoModel, 
    SiglipImageProcessor, # å°½ç®¡æ²¡ç›´æ¥ç”¨ï¼Œä½†ä¿æŒå¼•å…¥ä»¥é˜²ä¸‡ä¸€
    SiglipTokenizer, 
    Qwen2_5_VLForConditionalGeneration, 
    AutoProcessor
)
import opentimelineio as otio

# --- è·¯å¾„é…ç½® ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(BASE_DIR)
VIDEO_DIR = os.path.join(PROJECT_DIR, "data/demo-data")
OUTPUT_DIR = os.path.join(PROJECT_DIR, "output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

FEATURE_PATH = os.path.join(OUTPUT_DIR, "Mosaic_preview_features.npy")
MOSAIC_PREVIEW_DIR = os.path.join(OUTPUT_DIR, "mosaic_previews")  # ç”± 02 è„šæœ¬å†™å…¥
OUTPUT_OTIO = os.path.join(OUTPUT_DIR, "timeline.otio")

# --- æ¨¡å‹ ID ---
SIGLIP_ID = "google/siglip-so400m-patch14-384"
QWEN_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

def load_models():
    print("ğŸš€ [RTX 5090] loading dual model system...")
    
    # 1. åŠ è½½ SigLIP (é¿å¼€ AutoProcessor Bug)
    s_model = AutoModel.from_pretrained(SIGLIP_ID, torch_dtype=torch.bfloat16).to("cuda")
    s_tokenizer = SiglipTokenizer.from_pretrained(SIGLIP_ID)
    
    # 2. åŠ è½½ Qwen2.5-VL (ä½¿ç”¨ SDPA ç¡®ä¿ 5090 å…¼å®¹æ€§)
    # d_proc ç”¨äºå¤„ç† Qwen çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥
    d_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        QWEN_ID, 
        torch_dtype=torch.bfloat16, 
        attn_implementation="sdpa",
        device_map="auto"
    )
    d_proc = AutoProcessor.from_pretrained(QWEN_ID)
    
    return s_model, s_tokenizer, d_model, d_proc

def find_highlight_moments(text_query, features, s_model, s_tokenizer):
    """è¯­ä¹‰æœç´¢ï¼šæ‰¾å‡ºæœ€ç¬¦åˆæè¿°çš„ç§’æ•°"""
    print(f"ğŸ” semantic search keyword: '{text_query}'")
    
    inputs = s_tokenizer([text_query], return_tensors="pt", padding="max_length").to("cuda")
    
    with torch.no_grad():
        outputs = s_model.get_text_features(**inputs)
        # å…¼å®¹æ€§å¤„ç†ï¼šæå– Pooler Output
        text_emb = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs
        text_emb = text_emb / text_emb.norm(p=2, dim=-1, keepdim=True)

    # å…³é”®ç‚¹ï¼šå°†ç‰¹å¾è½¬ä¸º bfloat16 ä»¥åŒ¹é… 5090 ä¸Šçš„æ¨¡å‹è¾“å‡º
    features_tensor = torch.from_numpy(features).to("cuda", dtype=torch.bfloat16)
    similarities = (features_tensor @ text_emb.T).squeeze(1)
    
    # è·å–å¾—åˆ†æœ€é«˜çš„å‰ K ä¸ªç§’æ•°ç´¢å¼•
    # è¿™é‡Œæˆ‘ä»¬åªå–å¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªç§’æ•°ï¼Œä½œä¸ºæ ¸å¿ƒé«˜å…‰ç‚¹
    _, index = torch.topk(similarities, k=1)
    return index.cpu().item() # è¿”å›å•ä¸ªæœ€ä½³ç§’æ•°

def find_best_cam_with_qwen(mosaic_image_path, d_model, d_proc):
    if not os.path.exists(mosaic_image_path):
        print(f"âŒ error: mosaic image {mosaic_image_path} not found")
        return "def2_cam_00.mp4"
        
    print(f"ğŸ§ Qwen2.5-VL is analyzing: {mosaic_image_path}...")
    image = Image.open(mosaic_image_path).convert("RGB")
    
    prompt_text = (
        "In this 2x4 grid of camera views, which camera number provides the best close-up view "
        "of the slam dunk? Reply with ONLY the number (0-7)."
    )
    
    # ä¸¥æ ¼éµå¾ª Transformers è¦æ±‚çš„æ ¼å¼
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt_text},
            ],
        }
    ]

    # 1. ç”Ÿæˆæ¨¡æ¿æ–‡æœ¬
    text = d_proc.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # 2. ä½¿ç”¨ Processor åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ (5090 åŠ é€Ÿå…³é”®)
    inputs = d_proc(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    ).to(d_model.device)

    # 3. ç”Ÿæˆå†³ç­–
    with torch.no_grad():
        # æ³¨æ„ï¼šQwen2.5-VL ç”Ÿæˆæ—¶ä¸éœ€è¦æ‰‹åŠ¨ä¼  input_idsï¼Œç›´æ¥ä¼  inputs å±•å¼€å³å¯
        generated_ids = d_model.generate(**inputs, max_new_tokens=20)
        
        # åªéœ€è¦è·å–æ–°ç”Ÿæˆçš„ token
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        response_text = d_proc.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

    print(f"ğŸ¤– Qwen å›ç­”: {response_text}")

    # 4. è§£æç»“æœ (é€»è¾‘ä¿æŒä¸å˜)
    cam_map = {
        "0": "def2_cam_00.mp4", "1": "def2_cam_08.mp4", "2": "def2_cam_15.mp4", "3": "def2_cam_23.mp4",
        "4": "def2_cam_45.mp4", "5": "def2_cam_51.mp4", "6": "def2_cam_66.mp4", "7": "def2_cam_73.mp4"
    }
    
    import re
    match = re.search(r"(\d)", response_text)
    if match:
        cam_key = match.group(1)
        if cam_key in cam_map:
            return cam_map[cam_key]
    
    return "def2_cam_00.mp4"

def create_clean_otio(decisions, output_path, fps=25.0):
    """ç”Ÿæˆæ ‡å‡† OTIO æ—¶é—´è½´ï¼Œç¡®ä¿æ—¶é•¿ä¸¥æ ¼ä¸º 9 ç§’"""
    timeline = otio.schema.Timeline(name="RTX5090_AI_Edit")
    track = otio.schema.Track(name="Main", kind=otio.schema.TrackKind.Video)
    timeline.tracks.append(track)

    for d in decisions:
        # è®¡ç®—å¸§æ•°åŒºé—´
        start_frame = d['start'] * fps
        duration_frames = (d['end'] - d['start']) * fps
        
        media_ref = otio.schema.ExternalReference(
            target_url=os.path.abspath(os.path.join(VIDEO_DIR, d['cam']))
        )
        
        clip = otio.schema.Clip(
            name=f"Segment_{d['start']}s",
            media_reference=media_ref,
            source_range=otio.opentime.TimeRange(
                start_time=otio.opentime.RationalTime(start_frame, fps),
                duration=otio.opentime.RationalTime(duration_frames, fps)
            )
        )
        track.append(clip)

    otio.adapters.write_to_file(timeline, output_path)

def main():
    if not os.path.exists(FEATURE_PATH):
        print("âŒ error: feature file video_features.npy not found, please run 02 script first")
        return
    
    # ç¡®ä¿ mosaic_previews ç›®å½•å­˜åœ¨
    if not os.path.exists(MOSAIC_PREVIEW_DIR):
        print(f"âŒ error: mosaic preview directory {MOSAIC_PREVIEW_DIR} not found, please ensure 02 script has generated these preview images.")
        return

    # 1. åˆå§‹åŒ–
    features = np.load(FEATURE_PATH)
    s_model, s_tokenizer, d_model, d_proc = load_models()

    # 2. è¯­ä¹‰æœç´¢ï¼šæ‰¾åˆ°â€œæ‰£ç¯®â€å‘ç”Ÿçš„æ ¸å¿ƒé«˜å…‰ç§’æ•°
    query = "A player performing a slam dunk"
    # è¿™é‡Œæˆ‘ä»¬åªå–è¯­ä¹‰æœç´¢å¾—åˆ†æœ€é«˜çš„â€œä¸€ç§’â€ï¼Œä½œä¸º AI é‡ç‚¹å…³æ³¨çš„é«˜å…‰æ—¶é—´ç‚¹
    highlight_second = find_highlight_moments(query, features, s_model, s_tokenizer)
    print(f"âœ… SigLIP detected core highlight seconds: {highlight_second}")

    # 3. å¯¼æ¼”å†³ç­–é€»è¾‘ï¼šå°† 9 ç§’åˆ’åˆ†ä¸º 1 ç§’ä¸€ä¸ªçš„åŒºé—´è¿›è¡Œ AI å†³ç­–
    # è¿™ç§â€œå‘ä½â€é€»è¾‘ä¿è¯äº†è§†é¢‘æ—¶é•¿ç»å¯¹ä¸ä¼šç¿»å€
    total_duration_seconds = 9
    final_decisions = []
    
    for current_second in range(total_duration_seconds):
        mosaic_image_file = os.path.join(MOSAIC_PREVIEW_DIR, f"mosaic_preview_{current_second:04d}.png")
        
        # é»˜è®¤æœºä½
        best_cam = "def2_cam_00.mp4" 

        # æ ¸å¿ƒ AI å†³ç­–ï¼š
        # å¦‚æœå½“å‰ç§’æ˜¯æ ¸å¿ƒé«˜å…‰ç§’ï¼Œæˆ–è€…æ˜¯åœ¨å…¶é™„è¿‘çš„ä¸€ç§’ï¼Œ
        # åˆ™è°ƒç”¨ Qwen2.5-VL æ¥åˆ†æå¹¶é€‰æ‹©æœ€ä½³æœºä½ã€‚
        # å¦åˆ™ï¼ˆéé«˜å…‰æ—¶åˆ»ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å…¨æ™¯æœºä½ã€‚
        if abs(current_second - highlight_second) <= 1: # é«˜å…‰å‰åå„ä¸€ç§’ä¹Ÿè¿›è¡Œåˆ†æ
            print(f"âœ¨ entering AI director mode: analyzing highlight/nearby scene at {current_second} second...")
            best_cam = find_best_cam_with_qwen(mosaic_image_file, d_model, d_proc)
        else:
            print(f"Skip AI analysis for second {current_second}, using default cam: {best_cam}")

        final_decisions.append({
            "start": current_second,
            "end": current_second + 1, # æ¯æ®µæ—¶é•¿1ç§’
            "cam": best_cam
        })

    # 4. å¯¼å‡ºç»“æœ
    print(f"\nğŸ’¾ exporting OTIO timeline to: {OUTPUT_OTIO}")
    create_clean_otio(final_decisions, OUTPUT_OTIO)

if __name__ == "__main__":
    main()